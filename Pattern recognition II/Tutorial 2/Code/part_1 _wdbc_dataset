{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"part_1 _wdbc_dataset","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"vHSCJx9DE2--","colab_type":"code","colab":{}},"source":["# Code to mount your Google Drive\n","# Click the play button, then click the link, copy the password and paste it\n","# in the box area.\n","from google.colab import drive \n","\n","drive.mount(\"/content/gdrive\")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"rR-foa1CLXca","colab_type":"code","colab":{}},"source":["# Set as current directory \"gdrive/My Drive/AGH/Pattern Recognition\" path\n","import os\n","import pandas as pd\n","\n","os.chdir('gdrive/My Drive/AGH/Pattern Recognition')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"dYTRxewwMfxM","colab_type":"code","colab":{}},"source":["# Import the wdbc dataset\n","data = pd.read_csv(\"wdbc.csv\")\n","\n","# Printing the original Dataset\n","data"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"g8DhgJXfPJjA","colab_type":"code","colab":{}},"source":["# Removing features with low variance\n","# This is the first scikit method which can be found in the link below\n","# https://scikit-learn.org/stable/modules/feature_selection.html#removing-features-with-low-variance\n","\n","import statistics\n","from sklearn import preprocessing\n","\n","# Selecting only the features from the initial table\n","# We don't need Id number and Diagnosis\n","features=data.iloc[:,2:32] \n","\n","# Normalizing Data\n","min_max_scaler = preprocessing.MinMaxScaler()\n","features_scaled_in_array = min_max_scaler.fit_transform(features)\n","features_scaled = pd.DataFrame(features_scaled_in_array)\n","\n","# Calculating the variance of each feature\n","var = features_scaled.var(axis=0)\n","\n","# Setting a specific threshold of variance\n","threshold = 0.025\n","\n","# Dropping each feature(column) that has lower variance than our threshold.\n","for i in range(30):\n","  if var[i]<threshold:\n","    features_scaled.drop(columns=[i], axis = 1,inplace = True)\n","\n","# Print selected features\n","features_scaled "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"AQRd_dYugiaG","colab_type":"code","colab":{}},"source":["# Ordered diagram of the weights\n","\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","# Make the dataset:\n","list1 = var\n","list2 = (\t'0',\t'2',\t'5', '6',\t'7',\t'20',\t'21',\t'22',\t'26',\t'27'\t)\n","\n","# Order the lists in reference with the variance\n","list1, list2 = zip(*sorted(zip(list1, list2)))\n","y_pos = np.arange(len(list2))\n","\n","# Create bars\n","plt.bar(y_pos, list1)\n"," \n","# Create names on the x-axis\n","plt.xticks(y_pos, list2)\n"," \n","# Show graphic\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"cgtlmnXQnLo7","colab_type":"code","colab":{}},"source":["# Importing again the wdbc Dataset, to use the second method for feature selection\n","import os\n","import pandas as pd\n","\n","data = pd.read_csv(\"wdbc.csv\")\n","\n","# Printing original Dataset\n","data"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"0oDk_EH3nTsu","colab_type":"code","colab":{}},"source":["# Univariate feature selection\n","# This is the second scikit method which can be found in the link below\n","# https://scikit-learn.org/stable/modules/feature_selection.html#univariate-feature-selection\n","\n","\n","from sklearn.feature_selection import SelectKBest\n","from sklearn.feature_selection import *\n","\n","# Selecting only the features from the initial table \n","features=data.iloc[:,2:32] \n","\n","# Normalizing the data\n","min_max_scaler = preprocessing.MinMaxScaler()\n","X = min_max_scaler.fit_transform(features)\n","\n","# Printing Normalized Data\n","X\n","\n","# Chooding \"diagnosis\" as our target/label feature\n","y = data.iloc[:,1]\n","\n","# Using the second method to perform feature selection\n","# As a scoring/selection function you could use: chi2, f_classif, mutual_info_classif\n","# k is the number of features to be kept\n","X_new = SelectKBest(chi2, k=5).fit_transform(X,y) \n","\n","# Printing Selected Features\n","X_new"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"GIYY3tBcnm0j","colab_type":"code","colab":{}},"source":["# Ordered diagram of the weights\n","\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","\n","X_new = SelectKBest(chi2, k=2).fit(X, y)\n","scores = list(X_new.scores_)\n","\n","# Make the dataset:\n","list1 = scores\n","list2 = (\t'0',\t'2',\t'5', '6',\t'7',\t'20',\t'21',\t'22',\t'26',\t'27'\t)\n","\n","# Order the lists in reference with the variance\n","list1, list2 = zip(*sorted(zip(list1, list2)))\n","y_pos = np.arange(len(list2))\n","\n","# Create bars\n","plt.bar(y_pos, list1)\n"," \n","# Create names on the x-axis\n","plt.xticks(y_pos, list2)\n"," \n","# Show graphic\n","plt.show()"],"execution_count":0,"outputs":[]}]}