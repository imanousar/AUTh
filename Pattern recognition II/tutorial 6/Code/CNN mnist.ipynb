{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Importing libraries</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import keras\n",
    "from matplotlib import pyplot as plt\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras import backend as K\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>importing dataset</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv('mnist.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Reshaping the dataset</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (8000, 28, 28, 1)\n",
      "8000 train samples\n",
      "2000 test samples\n"
     ]
    }
   ],
   "source": [
    "# input image dimensions\n",
    "img_rows, img_cols = 28, 28\n",
    "\n",
    "n_instances=10000 ## for mnist training and fitting max 15000 intances is enough more than that is optional \n",
    "# and requires more performance and time\n",
    "X = data.iloc[0:n_instances,1:].values\n",
    "Y = data.iloc[0:n_instances,0].values\n",
    "test_size=0.20##test size\n",
    "\n",
    "x_train,x_test,y_train,y_test=train_test_split(X,Y,test_size=test_size,random_state=3)\n",
    "xx = np.expand_dims(x_train, axis=0)\n",
    "xt = np.expand_dims(x_test, axis=0)\n",
    "\n",
    "x_train = xx.reshape(len(x_train),28,28,1)##reshaping dataset into 4 dimensions\n",
    "x_test = xt.reshape(len(x_test),28,28,1)##reshaping dataset into 4 dimensions\n",
    "   \n",
    "input_shape = (img_rows, img_cols, 1)\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255 ##dividing each pixel by 255\n",
    "x_test /= 255 \n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "num_classes=10\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Fitting model</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/3\n",
      "8000/8000 [==============================] - 12s 1ms/step - loss: 0.8064 - accuracy: 0.7420 - val_loss: 0.3387 - val_accuracy: 0.9105\n",
      "Epoch 2/3\n",
      "8000/8000 [==============================] - 10s 1ms/step - loss: 0.2576 - accuracy: 0.9234 - val_loss: 0.2291 - val_accuracy: 0.9370\n",
      "Epoch 3/3\n",
      "8000/8000 [==============================] - 10s 1ms/step - loss: 0.1715 - accuracy: 0.9471 - val_loss: 0.2056 - val_accuracy: 0.9430\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "##activation is the activation function in each layer\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, kernel_size=(3, 3),\n",
    "                 activation='relu',\n",
    "                 input_shape=input_shape))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adadelta(),\n",
    "              metrics=['accuracy'])\n",
    "batch_size = 128\n",
    "#The batch size is a hyperparameter that defines the number of samples to work through before updating \n",
    "#the internal model parameters.\n",
    "epochs = 3 ##a number of epochs means how many times you go through your training set.\n",
    "##more epochs can get more accuracy but it will need more time and performance\n",
    "##note:for mnist dataset max amount of epochs needed is 4 and it can give around 94% accuracy \n",
    "#that is more than enough\n",
    "#By setting verbose 0, 1 or 2 you just say how do you want to 'see' the training progress for each epoch\n",
    "history=model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_data=(x_test, y_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Accuracy results</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])\n",
    "\n",
    "# summarize history for accuracy\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Image Results</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred=model.predict(x_test)\n",
    "numberof_images_display=5\n",
    "n= np.random.randint(0,len(x_test),numberof_images_display)\n",
    "for i in n:\n",
    "    two_d = (np.reshape(x_test[i], (28, 28)) * 255).astype(np.uint8)\n",
    "    print(\"Predicted value: \",np.argmax(pred[i],axis=None,out=None)) ##from binary to real value\n",
    "    print(\"Real value: \",np.argmax(y_test[i], axis=None, out=None))\n",
    "                                                \n",
    "    plt.imshow(two_d, interpolation='nearest',cmap='gray')\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
